{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nps4wTiS1aR"
      },
      "source": [
        "yfinance => Python library that allows you to download historical market data from Yahoo Finance. It is commonly used for stock price analysis, time series forecasting, and financial modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (0.2.55)\n",
            "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.31 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sakshi\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install yfinance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K_1JCxvOmkM",
        "outputId": "ae9ff4b8-8104-43c8-cd7a-1bea01778213"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Price           Close       High        Low       Open     Volume\n",
            "Ticker           AAPL       AAPL       AAPL       AAPL       AAPL\n",
            "Date                                                             \n",
            "2015-01-02  24.320423  24.789792  23.879972  24.778669  212818400\n",
            "2015-01-05  23.635284  24.169164  23.448427  24.089082  257142000\n",
            "2015-01-06  23.637506  23.897772  23.274912  23.699792  263188400\n",
            "2015-01-07  23.968962  24.069063  23.735389  23.846614  160423600\n",
            "2015-01-08  24.889910  24.947747  24.180294  24.298194  237458000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "# Download historical stock data (Example: AAPL - Apple)\n",
        "df = yf.download(\"AAPL\", start=\"2015-01-01\", end=\"2024-01-01\")\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Save dataset to CSV\n",
        "df.to_csv(\"AAPL_stock_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etI3amL5S-ss",
        "outputId": "fd64e0c9-d04f-417f-972d-b870feac8df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Price           Close       High        Low       Open     Volume\n",
            "Ticker           AAPL       AAPL       AAPL       AAPL       AAPL\n",
            "Date                                                             \n",
            "2015-01-02  24.320423  24.789792  23.879972  24.778669  212818400\n",
            "2015-01-05  23.635284  24.169164  23.448427  24.089082  257142000\n",
            "2015-01-06  23.637506  23.897772  23.274912  23.699792  263188400\n",
            "2015-01-07  23.968962  24.069063  23.735389  23.846614  160423600\n",
            "2015-01-08  24.889910  24.947747  24.180294  24.298194  237458000\n"
          ]
        }
      ],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKLDyEGWTtwg"
      },
      "source": [
        "Open → Opening price\n",
        "\n",
        "High → Highest price of the day\n",
        "\n",
        "Low → Lowest price of the day\n",
        "\n",
        "Close → Closing price of the stock\n",
        "\n",
        "Adj Close → Adjusted closing price (removes stock splits)\n",
        "\n",
        "Volume → Total number of shares traded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY7MqQNHTqPG",
        "outputId": "9b7978ed-fda4-442f-f629-4db60b07c5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2266 entries, Ticker to 2023-12-29\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Close   2265 non-null   object\n",
            " 1   High    2265 non-null   object\n",
            " 2   Low     2265 non-null   object\n",
            " 3   Open    2265 non-null   object\n",
            " 4   Volume  2265 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 106.2+ KB\n",
            "None\n",
            "                         Close                High                 Low  \\\n",
            "Price                                                                    \n",
            "Ticker                    AAPL                AAPL                AAPL   \n",
            "Date                       NaN                 NaN                 NaN   \n",
            "2015-01-02  24.320423126220703  24.789792071440623  23.879972027478615   \n",
            "2015-01-05  23.635284423828125  24.169164129068307    23.4484274604303   \n",
            "2015-01-06   23.63750648498535   23.89777223644289  23.274912374186805   \n",
            "\n",
            "                          Open     Volume  \n",
            "Price                                      \n",
            "Ticker                    AAPL       AAPL  \n",
            "Date                       NaN        NaN  \n",
            "2015-01-02    24.7786689089419  212818400  \n",
            "2015-01-05   24.08908208842444  257142000  \n",
            "2015-01-06  23.699792131779642  263188400  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"AAPL_stock_data.csv\", index_col=0)\n",
        "\n",
        "# Display dataset info\n",
        "print(df.info())\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP2p1ru6U6gt",
        "outputId": "dd036cb3-c2ab-42f6-f701-ef1feabe84ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                Close       High        Low       Open     Volume\n",
            "Date                                                             \n",
            "2015-01-02  24.320423  24.789792  23.879972  24.778669  212818400\n",
            "2015-01-05  23.635284  24.169164  23.448427  24.089082  257142000\n",
            "2015-01-06  23.637506  23.897772  23.274912  23.699792  263188400\n",
            "2015-01-07  23.968962  24.069063  23.735389  23.846614  160423600\n",
            "2015-01-08  24.889910  24.947747  24.180294  24.298194  237458000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 2264 entries, 2015-01-02 to 2023-12-29\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Close   2264 non-null   float64\n",
            " 1   High    2264 non-null   float64\n",
            " 2   Low     2264 non-null   float64\n",
            " 3   Open    2264 non-null   float64\n",
            " 4   Volume  2264 non-null   int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 106.1 KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset, skipping the first 3 rows\n",
        "df = pd.read_csv(\"AAPL_stock_data.csv\", skiprows=3, header=None)\n",
        "\n",
        "# Manually define correct column names\n",
        "df.columns = [\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
        "\n",
        "# Convert 'Date' to datetime format (YYYY-MM-DD format detected)\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "# Set 'Date' as index\n",
        "df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n",
        "print(df.info())  # Check if Date is properly formatted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVj3oOOQWGCT",
        "outputId": "0db2fdcb-2a83-4f87-db77-d4f1512a702c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Close     0\n",
            "High      0\n",
            "Low       0\n",
            "Open      0\n",
            "Volume    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Fill missing values using forward fill\n",
        "#df.fillna(method='ffill', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_1TLd7CWTHJ"
      },
      "source": [
        "Feature Engineering (Add Technical Indicators)\n",
        "We will create 7 new features:\n",
        "\n",
        "✅ Moving Averages: 10-day, 50-day SMA\n",
        "\n",
        "✅ Exponential Moving Average (EMA)\n",
        "\n",
        "✅ Volatility: Rolling standard deviation\n",
        "\n",
        "✅ Relative Strength Index (RSI)\n",
        "\n",
        "✅ Moving Average Convergence Divergence (MACD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3scJ9kWdWGMc",
        "outputId": "74490671-8acf-4431-e0ce-7dca24ced343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Close        High         Low        Open    Volume  \\\n",
            "Date                                                                   \n",
            "2023-12-22  192.444580  194.243775  191.818335  194.015137  37122800   \n",
            "2023-12-26  191.897873  192.732856  191.679185  192.454528  28919300   \n",
            "2023-12-27  191.997269  192.345186  189.949565  191.341219  48087700   \n",
            "2023-12-28  192.424713  193.498269  192.017156  192.981369  34049900   \n",
            "2023-12-29  191.380951  193.239786  190.585722  192.742770  42628800   \n",
            "\n",
            "                SMA_10      SMA_50      EMA_10  Volatility        RSI  \\\n",
            "Date                                                                    \n",
            "2023-12-22  194.578770  184.200591  193.793832    1.799725  59.246149   \n",
            "2023-12-26  194.565851  184.487575  193.449112    1.820423  49.031984   \n",
            "2023-12-27  194.410780  184.779127  193.185141    1.976150  52.291541   \n",
            "2023-12-28  193.975395  185.110400  193.046881    1.873469  47.920482   \n",
            "2023-12-29  193.420723  185.446807  192.743985    1.716830  40.185209   \n",
            "\n",
            "                MACD  Signal_Line  \n",
            "Date                               \n",
            "2023-12-22  2.642146     3.246319  \n",
            "2023-12-26  2.312633     3.059582  \n",
            "2023-12-27  2.036042     2.854874  \n",
            "2023-12-28  1.830236     2.649946  \n",
            "2023-12-29  1.564871     2.432931  \n"
          ]
        }
      ],
      "source": [
        "# Moving Averages\n",
        "df[\"SMA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
        "df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
        "\n",
        "# Exponential Moving Average\n",
        "df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "# Volatility (Rolling Standard Deviation)\n",
        "df[\"Volatility\"] = df[\"Close\"].rolling(window=10).std()\n",
        "\n",
        "# Relative Strength Index (RSI)\n",
        "def compute_rsi(series, window=14):\n",
        "    delta = series.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "df[\"RSI\"] = compute_rsi(df[\"Close\"], 14)\n",
        "\n",
        "# MACD (Moving Average Convergence Divergence)\n",
        "df[\"EMA_12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "df[\"EMA_26\"] = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "df[\"MACD\"] = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
        "df[\"Signal_Line\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(columns=[\"EMA_12\", \"EMA_26\"], inplace=True)\n",
        "\n",
        "print(df.tail())  # Check the new features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7b_SQKznRUA",
        "outputId": "bd9c8900-177c-4626-cf24-36324934e9f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Size: 1772, Testing Size: 443\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Drop NaN values from feature engineering\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define Features and Target\n",
        "features = [\"Open\", \"High\", \"Low\", \"Volume\", \"SMA_10\", \"SMA_50\", \"EMA_10\", \"Volatility\", \"RSI\", \"MACD\", \"Signal_Line\"]\n",
        "target = \"Close\"\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Train-Test Split (80-20)\n",
        "train_size = int(len(df) * 0.8)\n",
        "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "print(f\"Training Size: {len(X_train)}, Testing Size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from optuna) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.13.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from xgboost) (2.1.3)\n",
            "Requirement already satisfied: scipy in c:\\users\\sakshi\\anaconda3\\envs\\tf_env\\lib\\site-packages (from xgboost) (1.15.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18i9gPb-nzcn",
        "outputId": "f1ed8d9d-e93c-4cd5-9d03-49d7963a1e43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[I 2025-04-02 11:03:20,421] A new study created in memory with name: no-name-48fa7887-8302-4cab-8e87-d7c278324917\n",
            "[I 2025-04-02 11:03:22,633] Trial 0 finished with value: 4.31954244781563 and parameters: {'n_estimators': 1000, 'learning_rate': 0.04821314974904433, 'max_depth': 8, 'subsample': 0.9164371069336219, 'colsample_bytree': 0.722080614014309}. Best is trial 0 with value: 4.31954244781563.\n",
            "[I 2025-04-02 11:03:26,295] Trial 1 finished with value: 4.1829265999202 and parameters: {'n_estimators': 700, 'learning_rate': 0.03880449613090263, 'max_depth': 9, 'subsample': 0.932783872454809, 'colsample_bytree': 0.6450640112445923}. Best is trial 1 with value: 4.1829265999202.\n",
            "[I 2025-04-02 11:03:26,847] Trial 2 finished with value: 3.798523861990556 and parameters: {'n_estimators': 500, 'learning_rate': 0.01729538668286701, 'max_depth': 3, 'subsample': 0.9234204835017666, 'colsample_bytree': 0.7852666107772058}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:27,605] Trial 3 finished with value: 25.909295523408844 and parameters: {'n_estimators': 500, 'learning_rate': 0.003001529065044816, 'max_depth': 5, 'subsample': 0.8040384734465796, 'colsample_bytree': 0.9934278518918358}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:27,981] Trial 4 finished with value: 4.208453847915271 and parameters: {'n_estimators': 200, 'learning_rate': 0.055135621758846946, 'max_depth': 5, 'subsample': 0.8413880777234631, 'colsample_bytree': 0.6740955709081714}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:29,012] Trial 5 finished with value: 46.458187266881524 and parameters: {'n_estimators': 700, 'learning_rate': 0.001180218659017003, 'max_depth': 4, 'subsample': 0.6394794940528529, 'colsample_bytree': 0.918230523468045}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:30,397] Trial 6 finished with value: 20.51262194631331 and parameters: {'n_estimators': 500, 'learning_rate': 0.003584456344427215, 'max_depth': 9, 'subsample': 0.7103157519056575, 'colsample_bytree': 0.983232475005589}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:30,549] Trial 7 finished with value: 24.32888580391154 and parameters: {'n_estimators': 100, 'learning_rate': 0.015637727516139267, 'max_depth': 5, 'subsample': 0.8970917703454337, 'colsample_bytree': 0.8603455601673358}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:31,774] Trial 8 finished with value: 30.476570783834696 and parameters: {'n_estimators': 700, 'learning_rate': 0.001872043860562707, 'max_depth': 9, 'subsample': 0.7244533738537486, 'colsample_bytree': 0.9243336559344668}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:32,108] Trial 9 finished with value: 4.239653042692094 and parameters: {'n_estimators': 400, 'learning_rate': 0.04009634651688107, 'max_depth': 5, 'subsample': 0.891539740579377, 'colsample_bytree': 0.6093489252190433}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:32,313] Trial 10 finished with value: 7.83544806486866 and parameters: {'n_estimators': 300, 'learning_rate': 0.0105973026312258, 'max_depth': 3, 'subsample': 0.9694340899672891, 'colsample_bytree': 0.7808122379558048}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:33,365] Trial 11 finished with value: 4.141528575199722 and parameters: {'n_estimators': 700, 'learning_rate': 0.01585206204363267, 'max_depth': 7, 'subsample': 0.994504278001111, 'colsample_bytree': 0.7786840823624865}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:35,016] Trial 12 finished with value: 4.079285615184776 and parameters: {'n_estimators': 900, 'learning_rate': 0.019719668722017688, 'max_depth': 7, 'subsample': 0.9911783489278375, 'colsample_bytree': 0.7858576596716672}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:37,010] Trial 13 finished with value: 4.12560496104221 and parameters: {'n_estimators': 1000, 'learning_rate': 0.020678811124116188, 'max_depth': 7, 'subsample': 0.9664581287020593, 'colsample_bytree': 0.8337783731740265}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:37,655] Trial 14 finished with value: 4.374704847486509 and parameters: {'n_estimators': 900, 'learning_rate': 0.00557884841550365, 'max_depth': 3, 'subsample': 0.8507887869230943, 'colsample_bytree': 0.7244119170071862}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:38,878] Trial 15 finished with value: 4.493741433722709 and parameters: {'n_estimators': 800, 'learning_rate': 0.006600092478011059, 'max_depth': 6, 'subsample': 0.991917451738126, 'colsample_bytree': 0.7393065390356779}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:40,928] Trial 16 finished with value: 4.3172208041154505 and parameters: {'n_estimators': 400, 'learning_rate': 0.09642890428130027, 'max_depth': 10, 'subsample': 0.7605604383354817, 'colsample_bytree': 0.8365601083235108}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:41,859] Trial 17 finished with value: 4.167040512739401 and parameters: {'n_estimators': 600, 'learning_rate': 0.022786160670872933, 'max_depth': 6, 'subsample': 0.8627705099459279, 'colsample_bytree': 0.8789821954986052}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:44,092] Trial 18 finished with value: 4.17169766393795 and parameters: {'n_estimators': 900, 'learning_rate': 0.008974116272213601, 'max_depth': 8, 'subsample': 0.926646977261912, 'colsample_bytree': 0.7965458450125942}. Best is trial 2 with value: 3.798523861990556.\n",
            "[I 2025-04-02 11:03:44,642] Trial 19 finished with value: 3.9783850125211626 and parameters: {'n_estimators': 600, 'learning_rate': 0.02754110464531085, 'max_depth': 4, 'subsample': 0.9563973961280156, 'colsample_bytree': 0.6849301699224613}. Best is trial 2 with value: 3.798523861990556.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'n_estimators': 500, 'learning_rate': 0.01729538668286701, 'max_depth': 3, 'subsample': 0.9234204835017666, 'colsample_bytree': 0.7852666107772058}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=100),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "    }\n",
        "\n",
        "    model = XGBRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    return mean_absolute_error(y_test, preds)\n",
        "\n",
        "# Run Optuna for tuning\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Best Parameters\n",
        "best_params = study.best_params\n",
        "print(\"Best Parameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g-xOrk_nRey",
        "outputId": "ac30c3c4-95fc-4b96-8dfb-2df9c855c3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'n_estimators': 500, 'learning_rate': 0.01729538668286701, 'max_depth': 3, 'subsample': 0.9234204835017666, 'colsample_bytree': 0.7852666107772058}\n"
          ]
        }
      ],
      "source": [
        "best_params = study.best_params  # Get best hyperparameters from Optuna\n",
        "print(\"Best Parameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A552XLPyoPek",
        "outputId": "3bdedcb8-ca75-422c-e822-195e68437679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final XGBoost MAE: 3.7985\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Train the final model with best parameters\n",
        "final_xgb = XGBRegressor(**best_params)\n",
        "final_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "xgb_preds = final_xgb.predict(X_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae = mean_absolute_error(y_test, xgb_preds)\n",
        "print(f\"Final XGBoost MAE: {mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "N0brTvjcoljY",
        "outputId": "41f20726-085d-4b54-8118-4e4b217abc50"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = final_xgb.feature_importances_\n",
        "\n",
        "# Create DataFrame\n",
        "features_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
        "\n",
        "# Sort by importance\n",
        "features_df = features_df.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(features_df[\"Feature\"], features_df[\"Importance\"], color=\"royalblue\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"XGBoost Feature Importance\")\n",
        "plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tThe \"Open\" price is the most important feature, meaning XGBoost relies heavily on it for predictions.\n",
        "2.\tSMA_10 (Simple Moving Average - 10) and EMA_10 (Exponential Moving Average - 10) are also significant, indicating that short-term trends influence the model.\n",
        "3.\tHigh and Low prices contribute moderately, suggesting that price range plays a role.\n",
        "4.\tSMA_50 has some importance, but MACD, RSI, Volume, Signal_Line, and Volatility are not contributing much"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iX3X0QAo5ul",
        "outputId": "d6c00c8c-a913-4868-d6b8-5a20996048ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training shape: (1764, 10, 5), Test shape: (441, 10, 5)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select relevant features based on XGBoost importance\n",
        "selected_features = ['High', 'SMA_10', 'Open', 'Low', 'EMA_10']\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(df[selected_features])\n",
        "\n",
        "# Convert time series data into sequences\n",
        "def create_sequences(data, time_steps=10):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data[i:i+time_steps])  # Last `time_steps` observations\n",
        "        y.append(data[i+time_steps, 0]) # Predicting 'High' price\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Define time step (how many past days to use for prediction)\n",
        "time_steps = 10\n",
        "X_lstm, y_lstm = create_sequences(scaled_data, time_steps)\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_size = int(len(X_lstm) * 0.8)\n",
        "X_train, X_test = X_lstm[:train_size], X_lstm[train_size:]\n",
        "y_train, y_test = y_lstm[:train_size], y_lstm[train_size:]\n",
        "\n",
        "print(f\"Training shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "Num GPUs Available: 0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0514 - val_loss: 0.0331\n",
            "Epoch 2/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0261 - val_loss: 0.0378\n",
            "Epoch 3/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0263 - val_loss: 0.0291\n",
            "Epoch 4/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0226 - val_loss: 0.0397\n",
            "Epoch 5/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0237 - val_loss: 0.0234\n",
            "Epoch 6/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0227 - val_loss: 0.0235\n",
            "Epoch 7/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0211 - val_loss: 0.0613\n",
            "Epoch 8/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0212 - val_loss: 0.0302\n",
            "Epoch 9/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0196 - val_loss: 0.0358\n",
            "Epoch 10/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0202 - val_loss: 0.0248\n",
            "Epoch 11/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0187 - val_loss: 0.0464\n",
            "Epoch 12/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0225 - val_loss: 0.0203\n",
            "Epoch 13/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0198 - val_loss: 0.0236\n",
            "Epoch 14/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0203 - val_loss: 0.0217\n",
            "Epoch 15/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0203 - val_loss: 0.0639\n",
            "Epoch 16/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0199 - val_loss: 0.0371\n",
            "Epoch 17/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0172 - val_loss: 0.0341\n",
            "Epoch 18/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0172 - val_loss: 0.0187\n",
            "Epoch 19/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0176 - val_loss: 0.0263\n",
            "Epoch 20/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0210 - val_loss: 0.0330\n",
            "Epoch 21/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0195 - val_loss: 0.0543\n",
            "Epoch 22/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0184 - val_loss: 0.0364\n",
            "Epoch 23/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0173 - val_loss: 0.0194\n",
            "Epoch 24/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0165 - val_loss: 0.0343\n",
            "Epoch 25/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0182 - val_loss: 0.0403\n",
            "Epoch 26/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0185 - val_loss: 0.0194\n",
            "Epoch 27/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0179 - val_loss: 0.0206\n",
            "Epoch 28/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0187 - val_loss: 0.0217\n",
            "Epoch 29/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0169 - val_loss: 0.0317\n",
            "Epoch 30/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0172 - val_loss: 0.0265\n",
            "Epoch 31/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0183 - val_loss: 0.0504\n",
            "Epoch 32/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0182 - val_loss: 0.0213\n",
            "Epoch 33/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0175 - val_loss: 0.0350\n",
            "Epoch 34/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0174 - val_loss: 0.0296\n",
            "Epoch 35/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0179 - val_loss: 0.0328\n",
            "Epoch 36/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0178 - val_loss: 0.0179\n",
            "Epoch 37/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0180 - val_loss: 0.0166\n",
            "Epoch 38/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0195 - val_loss: 0.0163\n",
            "Epoch 39/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0181 - val_loss: 0.0222\n",
            "Epoch 40/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0175 - val_loss: 0.0350\n",
            "Epoch 41/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0184 - val_loss: 0.0168\n",
            "Epoch 42/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0172 - val_loss: 0.0390\n",
            "Epoch 43/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0179 - val_loss: 0.0340\n",
            "Epoch 44/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0180 - val_loss: 0.0767\n",
            "Epoch 45/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0197 - val_loss: 0.0199\n",
            "Epoch 46/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0179 - val_loss: 0.0209\n",
            "Epoch 47/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0165 - val_loss: 0.0336\n",
            "Epoch 48/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0162 - val_loss: 0.0519\n",
            "Epoch 49/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0185 - val_loss: 0.0281\n",
            "Epoch 50/50\n",
            "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0172 - val_loss: 0.0170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "#import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(time_steps, len(selected_features))),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)  # Output layer (Predicting stock price)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Save model\n",
        "model.save(\"lstm_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f94gVhv9pBy-",
        "outputId": "468940ae-cd1f-4b6f-9b22-c33ba1e58c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "LSTM MAE: 3.0214230903544137\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Reverse scaling\n",
        "predictions_rescaled = scaler.inverse_transform(np.hstack((predictions, np.zeros((len(predictions), len(selected_features)-1)))))[:, 0]\n",
        "y_test_rescaled = scaler.inverse_transform(np.hstack((y_test.reshape(-1,1), np.zeros((len(y_test), len(selected_features)-1)))))[:, 0]\n",
        "\n",
        "# Evaluate model\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "lstm_mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)\n",
        "print(f\"LSTM MAE: {lstm_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q6K8pdMqL-6",
        "outputId": "2668371c-d56b-4ed0-f141-e8ba9f59b9cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Close',\n",
              " 'High',\n",
              " 'Low',\n",
              " 'Open',\n",
              " 'Volume',\n",
              " 'SMA_10',\n",
              " 'SMA_50',\n",
              " 'EMA_10',\n",
              " 'Volatility',\n",
              " 'RSI',\n",
              " 'MACD',\n",
              " 'Signal_Line']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract column names before converting to NumPy\n",
        "feature_names = df.columns.tolist()\n",
        "feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5EN8We-quIz"
      },
      "source": [
        "If you are using XGBoost after LSTM, you need to flatten the input from 3D → 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSpzJnVdq-gS",
        "outputId": "7979160e-0eb9-4f94-eac5-757c6aae1a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM Input Shape: (1764, 10, 10)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Convert dataset into supervised learning format\n",
        "def create_sequences(data, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps):\n",
        "        X.append(data[i:i+n_steps, :])  # n_steps time windows\n",
        "        y.append(data[i+n_steps, 0])  # Predicting 'Close' price\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "n_steps = 10  # Window size\n",
        "features = ['Close', 'Open', 'High', 'Low', 'Volume', 'SMA_10', 'SMA_50', 'EMA_10', 'RSI', 'MACD']  # Use relevant features\n",
        "\n",
        "# Convert DataFrame to NumPy\n",
        "data = df[features].values\n",
        "X_lstm, y_lstm = create_sequences(data, n_steps)\n",
        "\n",
        "# Train-test split\n",
        "split = int(len(X_lstm) * 0.8)\n",
        "X_train_lstm, X_test_lstm = X_lstm[:split], X_lstm[split:]\n",
        "y_train_lstm, y_test_lstm = y_lstm[:split], y_lstm[split:]\n",
        "\n",
        "# Reshape X for LSTM (samples, timesteps, features)\n",
        "print(\"LSTM Input Shape:\", X_train_lstm.shape)  # Should be (samples, timesteps, features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_lstm: (2205, 10, 10)\n",
            "X_train_lstm shape after fix: (1764, 10, 10)\n"
          ]
        }
      ],
      "source": [
        "def create_sequences(data, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps):\n",
        "        X.append(data[i:i+n_steps, :])  # Use all features\n",
        "        y.append(data[i+n_steps, 0])    # Predicting 'Close' price\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Convert DataFrame to NumPy\n",
        "data = df[features].values\n",
        "X_lstm, y_lstm = create_sequences(data, n_steps)\n",
        "\n",
        "print(\"Shape of X_lstm:\", X_lstm.shape)  # Should be (samples, timesteps, features)\n",
        "\n",
        "# Ensure correct shape\n",
        "X_train_lstm = X_lstm[:split]\n",
        "X_test_lstm = X_lstm[split:]\n",
        "y_train_lstm = y_lstm[:split]\n",
        "y_test_lstm = y_lstm[split:]\n",
        "\n",
        "print(\"X_train_lstm shape after fix:\", X_train_lstm.shape)  # Should be (samples, timesteps, features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddnUt9azrBgy",
        "outputId": "0249a071-21af-4a9d-8325-b34e4605bfa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 5670.4185 - val_loss: 25328.6152\n",
            "Epoch 2/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5595.1279 - val_loss: 23903.4121\n",
            "Epoch 3/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5025.0190 - val_loss: 22629.8613\n",
            "Epoch 4/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4554.1997 - val_loss: 21262.0332\n",
            "Epoch 5/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4076.5369 - val_loss: 19799.6367\n",
            "Epoch 6/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3535.6934 - val_loss: 18316.5371\n",
            "Epoch 7/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3175.0435 - val_loss: 16850.5117\n",
            "Epoch 8/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2909.3672 - val_loss: 15488.4834\n",
            "Epoch 9/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2552.6243 - val_loss: 14258.5557\n",
            "Epoch 10/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2169.9214 - val_loss: 13201.4375\n",
            "Epoch 11/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2252.6582 - val_loss: 12286.2539\n",
            "Epoch 12/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1989.4314 - val_loss: 11589.5117\n",
            "Epoch 13/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1870.5708 - val_loss: 11071.6484\n",
            "Epoch 14/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2071.6602 - val_loss: 10717.2539\n",
            "Epoch 15/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1845.2318 - val_loss: 10449.8252\n",
            "Epoch 16/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1924.1315 - val_loss: 10250.2607\n",
            "Epoch 17/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2010.2616 - val_loss: 10080.8164\n",
            "Epoch 18/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1901.3104 - val_loss: 10034.4805\n",
            "Epoch 19/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1892.6167 - val_loss: 10031.0840\n",
            "Epoch 20/20\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1888.5509 - val_loss: 9954.5029\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "LSTM Features Shape: (1764, 1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "num_features = X_train.shape[2]\n",
        "\n",
        "num_features = X_train_lstm.shape[2]  # Ensure this is correct\n",
        "model_lstm = Sequential([\n",
        "    LSTM(64, return_sequences=True, activation=\"tanh\", input_shape=(n_steps, num_features)),\n",
        "    LSTM(32, return_sequences=False, activation=\"tanh\"),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(1, activation=\"linear\")\n",
        "])\n",
        "\n",
        "\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "model_lstm.fit(X_train_lstm, y_train_lstm, epochs=20, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))\n",
        "\n",
        "# Extract features from LSTM (hidden states)\n",
        "lstm_features_train = model_lstm.predict(X_train_lstm)  # Shape: (samples, 1)\n",
        "lstm_features_test = model_lstm.predict(X_test_lstm)\n",
        "\n",
        "print(\"LSTM Features Shape:\", lstm_features_train.shape)  # Should be (samples, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final XGBoost Input Shape: (1764, 11)\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Convert LSTM outputs into DataFrame\n",
        "lstm_train_df = pd.DataFrame(lstm_features_train, columns=['LSTM_Feature'])\n",
        "lstm_test_df = pd.DataFrame(lstm_features_test, columns=['LSTM_Feature'])\n",
        "\n",
        "# Original XGBoost Features (Latest Available Data)\n",
        "X_xgb = df.iloc[n_steps:-1][features].reset_index(drop=True)  # Offset by `n_steps`\n",
        "y_xgb = df.iloc[n_steps+1:]['Close'].reset_index(drop=True)  # Offset by `n_steps + 1`\n",
        "\n",
        "# Train-test split (consistent with LSTM)\n",
        "X_train_xgb, X_test_xgb = X_xgb[:split], X_xgb[split:]\n",
        "y_train_xgb, y_test_xgb = y_xgb[:split], y_xgb[split:]\n",
        "\n",
        "# Merge LSTM features with XGBoost features\n",
        "X_train_final = pd.concat([X_train_xgb.reset_index(drop=True), lstm_train_df], axis=1)\n",
        "X_test_final = pd.concat([X_test_xgb.reset_index(drop=True), lstm_test_df], axis=1)\n",
        "\n",
        "print(\"Final XGBoost Input Shape:\", X_train_final.shape)  # Should be (samples, features+1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exjncaHlrZg0",
        "outputId": "d70b7703-ce3e-450b-8927-14b259d67c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final XGBoost Input Shape (Train): (1764, 11), Labels Shape: (1764,)\n",
            "Final XGBoost Input Shape (Test): (440, 11), Labels Shape: (440,)\n"
          ]
        }
      ],
      "source": [
        "# Ensure LSTM feature size matches XGBoost's feature size\n",
        "min_samples = min(len(X_xgb), len(lstm_features_train))  # Find the smaller size\n",
        "\n",
        "X_xgb = X_xgb.iloc[:min_samples].reset_index(drop=True)\n",
        "y_xgb = y_xgb.iloc[:min_samples].reset_index(drop=True)\n",
        "lstm_features_train = lstm_features_train[:min_samples]  # Trim LSTM features\n",
        "lstm_features_test = lstm_features_test[:len(X_test_xgb)]  # Trim test set too\n",
        "\n",
        "# Merge XGBoost features with LSTM features\n",
        "X_train_final = pd.concat([X_train_xgb.reset_index(drop=True), pd.DataFrame(lstm_features_train, columns=['LSTM_Feature'])], axis=1)\n",
        "X_test_final = pd.concat([X_test_xgb.reset_index(drop=True), pd.DataFrame(lstm_features_test, columns=['LSTM_Feature'])], axis=1)\n",
        "\n",
        "print(f\"Final XGBoost Input Shape (Train): {X_train_final.shape}, Labels Shape: {y_train_xgb.shape}\")\n",
        "print(f\"Final XGBoost Input Shape (Test): {X_test_final.shape}, Labels Shape: {y_test_xgb.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H742yf0rIjz",
        "outputId": "1dd3dcd4-555a-4494-a57e-3fa6cc99b704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hybrid Model MAE: 6.0555\n"
          ]
        }
      ],
      "source": [
        "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=5)\n",
        "xgb_model.fit(X_train_final, y_train_xgb)\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test_final)\n",
        "\n",
        "# Evaluate Model\n",
        "mae = mean_absolute_error(y_test_xgb, y_pred_xgb)\n",
        "print(f\"Hybrid Model MAE: {mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfS623H-ta5m",
        "outputId": "8b35fe88-6a9c-40e3-f110-98bba3dd843f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.2474 - val_loss: 0.0888\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0878 - val_loss: 0.0876\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0881 - val_loss: 0.0868\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0882 - val_loss: 0.0870\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0880 - val_loss: 0.0864\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0884 - val_loss: 0.0850\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0853 - val_loss: 0.0851\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0829 - val_loss: 0.0842\n",
            "Epoch 9/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0829 - val_loss: 0.0851\n",
            "Epoch 10/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0828 - val_loss: 0.0858\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Hybrid Model MAE: 0.2508\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ✅ Step 1: Prepare Data\n",
        "X_train = np.random.rand(1000, 10, 1)  # Example: 1000 samples, 10 time-steps, 1 feature\n",
        "X_test = np.random.rand(200, 10, 1)\n",
        "y_train = np.random.rand(1000)\n",
        "y_test = np.random.rand(200)\n",
        "\n",
        "# ✅ Step 2: Define LSTM Model with Explicit Input Layer\n",
        "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]), name=\"lstm_input\")\n",
        "lstm_output = LSTM(64, return_sequences=False, name=\"lstm_features\")(input_layer)\n",
        "output_layer = Dense(1, activation=\"linear\")(lstm_output)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# ✅ Step 3: Compile & Train LSTM Model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# ✅ Step 4: Extract LSTM Features\n",
        "feature_extractor = Model(inputs=model.input, outputs=model.get_layer(\"lstm_features\").output)\n",
        "\n",
        "# 🔥 Fix: Run a Dummy Forward Pass to Initialize Model\n",
        "_ = model.predict(X_train[:1])  # Ensures model is initialized\n",
        "\n",
        "# Extract Features\n",
        "lstm_features_train = feature_extractor.predict(X_train)  # Shape: (num_samples, lstm_units)\n",
        "lstm_features_test = feature_extractor.predict(X_test)\n",
        "\n",
        "# Convert to 2D for XGBoost\n",
        "lstm_features_train = lstm_features_train.reshape(lstm_features_train.shape[0], -1)\n",
        "lstm_features_test = lstm_features_test.reshape(lstm_features_test.shape[0], -1)\n",
        "\n",
        "# ✅ Step 5: Train XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=6)\n",
        "xgb_model.fit(lstm_features_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "xgb_preds = xgb_model.predict(lstm_features_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "hybrid_mae = mean_absolute_error(y_test, xgb_preds)\n",
        "print(f\"Hybrid Model MAE: {hybrid_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iY8qKbOtdoL",
        "outputId": "8ad4eb8d-325a-4651-cc8b-ae2e0802caac"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Adam' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [32], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_layer, outputs\u001b[38;5;241m=\u001b[39moutput_layer)\n\u001b[1;32m---> 12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mAdam\u001b[49m(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train LSTM\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Adam' is not defined"
          ]
        }
      ],
      "source": [
        "# Define LSTM Model\n",
        "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "x = LSTM(128, return_sequences=True, name=\"lstm_layer\")(input_layer)\n",
        "x = LSTM(64, return_sequences=False)(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "output_layer = Dense(1)(x)\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "# Train LSTM\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdzKeIHGwHNc",
        "outputId": "5229388c-80fc-45ea-8ec6-2ec70d6560b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_layer_12\n",
            "lstm_layer\n",
            "lstm_20\n",
            "dropout_7\n",
            "dense_22\n",
            "dense_23\n"
          ]
        }
      ],
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Functional name=functional_16, built=True>"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_extractor = Model(inputs=model.input, outputs=model.get_layer(\"lstm_layer\").output)\n",
        "feature_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "LSTM Feature Shape: (1000, 10, 128)\n"
          ]
        }
      ],
      "source": [
        "lstm_features = feature_extractor.predict(X_train)\n",
        "print(\"LSTM Feature Shape:\", lstm_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_lstm = lstm_features.reshape(X_train.shape[0], -1)\n",
        "X_test_lstm = X_test_lstm.reshape(X_test_lstm.shape[0], -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_lstm shape: (1000, 1280)\n",
            "X_test_lstm shape: (441, 100)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train_lstm shape:\", X_train_lstm.shape)  # Should be (samples, features)\n",
        "print(\"X_test_lstm shape:\", X_test_lstm.shape)    # Should match X_train_lstm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n"
          ]
        }
      ],
      "source": [
        "feature_extractor = Model(inputs=model.input, outputs=model.get_layer(\"lstm_layer\").output)\n",
        "\n",
        "X_train_lstm = feature_extractor.predict(X_train).reshape(X_train.shape[0], -1)\n",
        "X_test_lstm = feature_extractor.predict(X_test).reshape(X_test.shape[0], -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_lstm shape: (1000, 1280)\n",
            "X_test_lstm shape: (200, 1280)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train_lstm shape:\", X_train_lstm.shape)\n",
        "print(\"X_test_lstm shape:\", X_test_lstm.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj3wf_AHtflD",
        "outputId": "df9fb70d-3c35-40ee-9392-24d36e18f9f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hybrid Model MAE: 0.2463\n"
          ]
        }
      ],
      "source": [
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=500,   # Increase estimators\n",
        "    learning_rate=0.03, # Lower learning rate\n",
        "    max_depth=7,        # Increase depth slightly\n",
        "    subsample=0.9,      # Slightly higher subsampling\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_lstm, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_model.predict(X_test_lstm)\n",
        "\n",
        "# Evaluate performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Hybrid Model MAE: {mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJEpoBpE0jWG",
        "outputId": "699e757b-5bc7-43a3-c799-8f8b6a904509"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['xgboost_model.pkl']"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "import tensorflow as tf\n",
        "\n",
        "# Save LSTM model in the new Keras format\n",
        "model.save(\"hybrid_lstm_xgboost.keras\")  # Instead of .h5\n",
        "\n",
        "# Save XGBoost model\n",
        "joblib.dump(xgb_model, \"xgboost_model.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'AAPL'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Normalize Data (for better LSTM training)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 17\u001b[0m df[features] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m df[target] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df[[target]])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# ✅ Create Sequences for LSTM\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
            "File \u001b[1;32mc:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\generic.py:2153\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\n\u001b[0;32m   2150\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   2152\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 2153\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2155\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2156\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[0;32m   2158\u001b[0m     ):\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n\u001b[0;32m   2160\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m astype_is_view(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(\n\u001b[0;32m   2161\u001b[0m             values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m   2162\u001b[0m         ):\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'AAPL'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# Normalize Data (for better LSTM training)\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "df[target] = scaler.fit_transform(df[[target]])\n",
        "\n",
        "# ✅ Create Sequences for LSTM\n",
        "def create_sequences(data, target_col, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps):\n",
        "        X.append(data[i : i + n_steps])  # Past `n_steps` data\n",
        "        y.append(data[i + n_steps, target_col])  # Target is `Close`\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "n_steps = 10  # Use 10 past days for predictions\n",
        "feature_cols = df[features].values  # Use selected stock features\n",
        "\n",
        "X, y = create_sequences(feature_cols, target_col=df.columns.get_loc(target), n_steps=n_steps)\n",
        "\n",
        "# ✅ Train-Test Split\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# ✅ Define & Train LSTM Model\n",
        "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]), name=\"lstm_input\")\n",
        "lstm_output = LSTM(64, return_sequences=False, name=\"lstm_features\")(input_layer)\n",
        "output_layer = Dense(1, activation=\"linear\")(lstm_output)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# ✅ Extract LSTM Features\n",
        "feature_extractor = Model(inputs=model.input, outputs=model.get_layer(\"lstm_features\").output)\n",
        "\n",
        "lstm_features_train = feature_extractor.predict(X_train)\n",
        "lstm_features_test = feature_extractor.predict(X_test)\n",
        "\n",
        "# Convert to 2D for XGBoost\n",
        "lstm_features_train = lstm_features_train.reshape(lstm_features_train.shape[0], -1)\n",
        "lstm_features_test = lstm_features_test.reshape(lstm_features_test.shape[0], -1)\n",
        "\n",
        "# ✅ Train XGBoost Model (Only LSTM Features)\n",
        "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=6)\n",
        "xgb_model.fit(lstm_features_train, y_train)\n",
        "\n",
        "# Predictions & Evaluation\n",
        "xgb_preds = xgb_model.predict(lstm_features_test)\n",
        "hybrid_mae = mean_absolute_error(y_test, xgb_preds)\n",
        "\n",
        "print(f\"🚀 Hybrid Model MAE (Real Stock Data): {hybrid_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (960, 10, 1), X_test shape: (240, 10, 1)\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# ✅ 1. Import Dependencies\n",
        "# ========================\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# ========================\n",
        "# ✅ 2. Data Preparation\n",
        "# ========================\n",
        "# Simulating example stock price data (Use real dataset here)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(1200, 10, 1)  # 1200 samples, 10 time-steps, 1 feature\n",
        "y = np.random.rand(1200)  # Target variable (stock closing price)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost-Only Model MAE: 0.2738\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# ✅ 3. XGBoost-Only Model\n",
        "# ========================\n",
        "# Flatten the 3D input data to 2D for XGBoost\n",
        "X_train_xgb = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_xgb = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Define XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.04,\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train XGBoost Model\n",
        "xgb_model.fit(X_train_xgb, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
        "\n",
        "# Evaluate\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost-Only Model MAE: {mae_xgb:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.1692 - val_loss: 0.0834\n",
            "Epoch 2/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0851 - val_loss: 0.0796\n",
            "Epoch 3/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0865 - val_loss: 0.0788\n",
            "Epoch 4/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0826 - val_loss: 0.0797\n",
            "Epoch 5/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0881 - val_loss: 0.0797\n",
            "Epoch 6/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0824 - val_loss: 0.0794\n",
            "Epoch 7/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0857 - val_loss: 0.0781\n",
            "Epoch 8/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0839 - val_loss: 0.0787\n",
            "Epoch 9/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0845 - val_loss: 0.0815\n",
            "Epoch 10/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0812 - val_loss: 0.0788\n",
            "Epoch 11/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0843 - val_loss: 0.0786\n",
            "Epoch 12/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0835 - val_loss: 0.0779\n",
            "Epoch 13/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0876 - val_loss: 0.0784\n",
            "Epoch 14/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0864 - val_loss: 0.0786\n",
            "Epoch 15/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0810 - val_loss: 0.0777\n",
            "Epoch 16/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0838 - val_loss: 0.0783\n",
            "Epoch 17/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0818 - val_loss: 0.0782\n",
            "Epoch 18/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0847 - val_loss: 0.0778\n",
            "Epoch 19/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0830 - val_loss: 0.0787\n",
            "Epoch 20/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0823 - val_loss: 0.0777\n",
            "Epoch 21/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0781 - val_loss: 0.0779\n",
            "Epoch 22/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0833 - val_loss: 0.0778\n",
            "Epoch 23/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0863 - val_loss: 0.0836\n",
            "Epoch 24/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0834 - val_loss: 0.0778\n",
            "Epoch 25/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0791 - val_loss: 0.0777\n",
            "Epoch 26/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0821 - val_loss: 0.0781\n",
            "Epoch 27/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0800 - val_loss: 0.0783\n",
            "Epoch 28/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0812 - val_loss: 0.0784\n",
            "Epoch 29/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0804 - val_loss: 0.0781\n",
            "Epoch 30/30\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0808 - val_loss: 0.0785\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n",
            "LSTM-Only Model MAE: 0.2712\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# ✅ 4. LSTM-Only Model\n",
        "# ========================\n",
        "\n",
        "# Step 1: Define LSTM Model\n",
        "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]), name=\"lstm_input\")\n",
        "\n",
        "# LSTM Layers\n",
        "x = LSTM(128, return_sequences=True, name=\"lstm_layer\")(input_layer)\n",
        "x = Dropout(0.2)(x)  # Dropout after LSTM layer for regularization\n",
        "x = LSTM(64, return_sequences=False, name=\"lstm_20\")(x)\n",
        "\n",
        "# Dense layers\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "output_layer = Dense(1)(x)\n",
        "\n",
        "# Compile Model\n",
        "lstm_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "lstm_model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "# Train LSTM Model\n",
        "lstm_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Predict with LSTM\n",
        "y_pred_lstm = lstm_model.predict(X_test).flatten()\n",
        "\n",
        "# Evaluate\n",
        "mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
        "print(f\"LSTM-Only Model MAE: {mae_lstm:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Final LSTM Feature Shape (Train): (960, 64)\n",
            "Final LSTM Feature Shape (Test): (240, 64)\n",
            "Hybrid Model (LSTM + XGBoost) MAE: 0.2699\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# ✅ 5. Hybrid Approach (XGBoost + LSTM)\n",
        "# ========================\n",
        "\n",
        "# Extract Features from the FINAL LSTM layer\n",
        "feature_extractor = Model(inputs=lstm_model.input, outputs=lstm_model.get_layer(\"lstm_20\").output)\n",
        "\n",
        "# Forward Pass to Extract Features\n",
        "lstm_features_train = feature_extractor.predict(X_train)\n",
        "lstm_features_test = feature_extractor.predict(X_test)\n",
        "\n",
        "# Reshape extracted features to 2D for XGBoost\n",
        "X_train_lstm = lstm_features_train.reshape(lstm_features_train.shape[0], -1)\n",
        "X_test_lstm = lstm_features_test.reshape(lstm_features_test.shape[0], -1)\n",
        "\n",
        "print(\"Final LSTM Feature Shape (Train):\", X_train_lstm.shape)\n",
        "print(\"Final LSTM Feature Shape (Test):\", X_test_lstm.shape)\n",
        "\n",
        "# Define Hybrid XGBoost Model\n",
        "xgb_model_hybrid = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.04,\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train XGBoost on LSTM Features\n",
        "xgb_model_hybrid.fit(X_train_lstm, y_train)\n",
        "\n",
        "# Predict with Hybrid Model\n",
        "y_pred_hybrid = xgb_model_hybrid.predict(X_test_lstm)\n",
        "\n",
        "# Evaluate\n",
        "mae_hybrid = mean_absolute_error(y_test, y_pred_hybrid)\n",
        "print(f\"Hybrid Model (LSTM + XGBoost) MAE: {mae_hybrid:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💡 **Performance Comparison:**\n",
            "📌 XGBoost-Only MAE: 0.2738\n",
            "📌 LSTM-Only MAE: 0.2712\n",
            "📌 Hybrid (LSTM + XGBoost) MAE: 0.2699\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================\n",
        "# ✅ 6. Compare Performance\n",
        "# ========================\n",
        "print(\"\\n💡 **Performance Comparison:**\")\n",
        "print(f\"📌 XGBoost-Only MAE: {mae_xgb:.4f}\")\n",
        "print(f\"📌 LSTM-Only MAE: {mae_lstm:.4f}\")\n",
        "print(f\"📌 Hybrid (LSTM + XGBoost) MAE: {mae_hybrid:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                Close       High        Low       Open     Volume\n",
            "Date                                                             \n",
            "2015-01-02  24.320423  24.789792  23.879972  24.778669  212818400\n",
            "2015-01-05  23.635284  24.169164  23.448427  24.089082  257142000\n",
            "2015-01-06  23.637506  23.897772  23.274912  23.699792  263188400\n",
            "2015-01-07  23.968962  24.069063  23.735389  23.846614  160423600\n",
            "2015-01-08  24.889910  24.947747  24.180294  24.298194  237458000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 2264 entries, 2015-01-02 to 2023-12-29\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Close   2264 non-null   float64\n",
            " 1   High    2264 non-null   float64\n",
            " 2   Low     2264 non-null   float64\n",
            " 3   Open    2264 non-null   float64\n",
            " 4   Volume  2264 non-null   int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 106.1 KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# ✅ 1. Import Required Libraries\n",
        "# ==============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==============================\n",
        "# ✅ 2. Load and Preprocess Data\n",
        "# ==============================\n",
        "\n",
        "# Load dataset, skipping the first 3 rows\n",
        "df = pd.read_csv(\"AAPL_stock_data.csv\", skiprows=3, header=None)\n",
        "\n",
        "# Manually define correct column names\n",
        "df.columns = [\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
        "\n",
        "# Convert 'Date' to datetime format (YYYY-MM-DD format detected)\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "# Set 'Date' as index\n",
        "df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n",
        "print(df.info()) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               Close      High       Low      Open    Volume    SMA_10  \\\n",
            "Date                                                                     \n",
            "2023-12-22  0.974564  0.976417  0.977154  0.983982  0.020927  0.986673   \n",
            "2023-12-26  0.971463  0.967902  0.976361  0.975127  0.007796  0.986600   \n",
            "2023-12-27  0.972027  0.965717  0.966497  0.968810  0.038477  0.985720   \n",
            "2023-12-28  0.974452  0.972215  0.978288  0.978116  0.016008  0.983250   \n",
            "2023-12-29  0.968530  0.970759  0.970125  0.976762  0.029739  0.980103   \n",
            "\n",
            "              SMA_50    EMA_10  Volatility        RSI      MACD  Signal_Line  \n",
            "Date                                                                          \n",
            "2023-12-22  0.927791  0.982220    0.010211  59.246149  0.014991     0.018419  \n",
            "2023-12-26  0.929419  0.980264    0.010328  49.031984  0.013121     0.017359  \n",
            "2023-12-27  0.931073  0.978766    0.011212  52.291541  0.011552     0.016198  \n",
            "2023-12-28  0.932953  0.977982    0.010629  47.920482  0.010384     0.015035  \n",
            "2023-12-29  0.934861  0.976263    0.009741  40.185209  0.008879     0.013804  \n"
          ]
        }
      ],
      "source": [
        "# Moving Averages\n",
        "df[\"SMA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
        "df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
        "\n",
        "# Exponential Moving Average\n",
        "df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "# Volatility (Rolling Standard Deviation)\n",
        "df[\"Volatility\"] = df[\"Close\"].rolling(window=10).std()\n",
        "\n",
        "# Relative Strength Index (RSI)\n",
        "def compute_rsi(series, window=14):\n",
        "    delta = series.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "df[\"RSI\"] = compute_rsi(df[\"Close\"], 14)\n",
        "\n",
        "# MACD (Moving Average Convergence Divergence)\n",
        "df[\"EMA_12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "df[\"EMA_26\"] = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "df[\"MACD\"] = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
        "df[\"Signal_Line\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(columns=[\"EMA_12\", \"EMA_26\"], inplace=True)\n",
        "\n",
        " # Check the new features\n",
        "df = df.sort_index()\n",
        "print(df.tail()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Moving Averages\n",
        "df[\"SMA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
        "df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
        "\n",
        "# Exponential Moving Average\n",
        "df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "# Volatility (Rolling Standard Deviation)\n",
        "df[\"Volatility\"] = df[\"Close\"].rolling(window=10).std()\n",
        "\n",
        "# Relative Strength Index (RSI)\n",
        "def compute_rsi(series, window=14):\n",
        "    delta = series.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "df[\"RSI\"] = compute_rsi(df[\"Close\"], 14)\n",
        "\n",
        "# MACD (Moving Average Convergence Divergence)\n",
        "df[\"EMA_12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "df[\"EMA_26\"] = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "df[\"MACD\"] = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
        "df[\"Signal_Line\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(columns=[\"EMA_12\", \"EMA_26\"], inplace=True)\n",
        "\n",
        "# Drop rows with NaN values caused by rolling calculations\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler()\n",
        "feature_columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\", \"SMA_10\", \"SMA_50\", \"EMA_10\", \"Volatility\", \"RSI\", \"MACD\", \"Signal_Line\"]\n",
        "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
        "\n",
        "# Define target variable (next day's Close price)\n",
        "target = \"Close\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(data, target_column, seq_length=10):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data.iloc[i:i+seq_length].values)\n",
        "        y.append(data.iloc[i+seq_length][target_column])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 10\n",
        "X, y = create_sequences(df[feature_columns], target, sequence_length)\n",
        "\n",
        "# Train-test split (80-20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost MAE: 0.0407\n"
          ]
        }
      ],
      "source": [
        "# Reshape X for XGBoost\n",
        "X_train_xgb = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_xgb = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Initialize and train XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=7,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_xgb, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sakshi\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 0.0104 - val_loss: 0.0025\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.1264e-04 - val_loss: 0.0037\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3.1391e-04 - val_loss: 0.0019\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 4.9940e-04 - val_loss: 0.0041\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.5126e-04 - val_loss: 0.0026\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2.8049e-04 - val_loss: 0.0014\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.1995e-04 - val_loss: 0.0040\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.0216e-04 - val_loss: 0.0035\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.9306e-04 - val_loss: 0.0027\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.2116e-04 - val_loss: 0.0015\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.7494e-04 - val_loss: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.0769e-04 - val_loss: 0.0031\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.5748e-04 - val_loss: 0.0023\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0747e-04 - val_loss: 0.0019\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.1209e-04 - val_loss: 0.0044\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.9241e-04 - val_loss: 0.0019\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2.2703e-04 - val_loss: 0.0020\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3.0246e-04 - val_loss: 0.0011\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2.5098e-04 - val_loss: 0.0064\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2.2761e-04 - val_loss: 0.0011\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.8388e-04 - val_loss: 0.0047\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2.1705e-04 - val_loss: 0.0017\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2.2519e-04 - val_loss: 0.0023\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.0834e-04 - val_loss: 0.0066\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3.1655e-04 - val_loss: 0.0042\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.5692e-04 - val_loss: 0.0027\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.9218e-04 - val_loss: 0.0024\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2.1573e-04 - val_loss: 0.0024\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.5187e-04 - val_loss: 0.0013\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.7807e-04 - val_loss: 0.0043\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "LSTM MAE: 0.0882\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# ✅ 4. LSTM-Only Model\n",
        "# ==============================\n",
        "\n",
        "# Define LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.1),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "# Train LSTM\n",
        "lstm_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_lstm = lstm_model.predict(X_test)\n",
        "mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
        "print(f\"LSTM MAE: {mae_lstm:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m12\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m12,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m26\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,101</span> (133.21 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,101\u001b[0m (133.21 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,101</span> (133.21 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,101\u001b[0m (133.21 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "\n",
        "# Define input explicitly using Functional API\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "x = LSTM(50, return_sequences=True)(inputs)\n",
        "x = LSTM(50, return_sequences=False)(x)\n",
        "lstm_features = Dense(25, activation='relu')(x)  # Extractable features\n",
        "outputs = Dense(1)(lstm_features)\n",
        "\n",
        "# Create the full LSTM model\n",
        "lstm_model = Model(inputs, outputs)\n",
        "\n",
        "# Compile and call the model once\n",
        "lstm_model.compile(optimizer='adam', loss='mse')\n",
        "lstm_model.summary()  # Debugging step\n",
        "\n",
        "# Now define feature extractor correctly\n",
        "feature_extractor = Model(inputs=lstm_model.input, outputs=lstm_features)\n",
        "\n",
        "# Extract LSTM features\n",
        "X_train_lstm_features = feature_extractor.predict(X_train)\n",
        "X_test_lstm_features = feature_extractor.predict(X_test)\n",
        "\n",
        "# Flatten features for XGBoost\n",
        "X_train_hybrid = X_train_lstm_features.reshape(X_train_lstm_features.shape[0], -1)\n",
        "X_test_hybrid = X_test_lstm_features.reshape(X_test_lstm_features.shape[0], -1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<KerasTensor shape=(None, 10, 12), dtype=float32, sparse=False, ragged=False, name=keras_tensor_40>\n",
            "<KerasTensor shape=(None, 25), dtype=float32, sparse=False, ragged=False, name=keras_tensor_43>\n"
          ]
        }
      ],
      "source": [
        "print(lstm_model.input)\n",
        "print(lstm_model.layers[-2].output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0071 - val_loss: 0.0080\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4719e-04 - val_loss: 0.0067\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4901e-04 - val_loss: 0.0029\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.6993e-04 - val_loss: 0.0051\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4902e-04 - val_loss: 0.0026\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.7047e-04 - val_loss: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.1471e-04 - val_loss: 0.0018\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3607e-04 - val_loss: 0.0024\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4152e-04 - val_loss: 0.0017\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8298e-04 - val_loss: 9.8835e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8833e-04 - val_loss: 8.8623e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3396e-04 - val_loss: 0.0016\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2886e-04 - val_loss: 0.0026\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.6175e-04 - val_loss: 0.0018\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3694e-04 - val_loss: 0.0027\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4903e-04 - val_loss: 0.0028\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5256e-04 - val_loss: 0.0015\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3184e-04 - val_loss: 0.0016\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3054e-04 - val_loss: 0.0021\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0830e-04 - val_loss: 0.0018\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1961e-04 - val_loss: 0.0013\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2282e-04 - val_loss: 0.0031\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0424e-04 - val_loss: 0.0035\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2819e-04 - val_loss: 0.0013\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.5761e-04 - val_loss: 0.0016\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3424e-04 - val_loss: 0.0021\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3612e-04 - val_loss: 0.0017\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.7992e-05 - val_loss: 0.0033\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4349e-04 - val_loss: 0.0026\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.3598e-04 - val_loss: 0.0028\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Hybrid Model MAE: 0.0457\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Ensure LSTM is trained before extracting features\n",
        "lstm_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Call the model on a dummy input to initialize it\n",
        "_ = lstm_model.predict(X_train[:1])\n",
        "\n",
        "# Make sure the model is built before extracting features\n",
        "lstm_model.build(input_shape=(None, X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "# Now create the feature extractor\n",
        "feature_extractor = Model(inputs=lstm_model.input, outputs=lstm_model.layers[-2].output)\n",
        "\n",
        "# Extract features from LSTM\n",
        "X_train_lstm_features = feature_extractor.predict(X_train)\n",
        "X_test_lstm_features = feature_extractor.predict(X_test)\n",
        "\n",
        "# Flatten the extracted features for XGBoost\n",
        "X_train_hybrid = X_train_lstm_features.reshape(X_train_lstm_features.shape[0], -1)\n",
        "X_test_hybrid = X_test_lstm_features.reshape(X_test_lstm_features.shape[0], -1)\n",
        "\n",
        "# Train XGBoost on LSTM features\n",
        "xgb_hybrid = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=7,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_hybrid.fit(X_train_hybrid, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_hybrid = xgb_hybrid.predict(X_test_hybrid)\n",
        "mae_hybrid = mean_absolute_error(y_test, y_pred_hybrid)\n",
        "print(f\"Hybrid Model MAE: {mae_hybrid:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "Hybrid Model MAE: 0.0407\n"
          ]
        }
      ],
      "source": [
        "# Extract features from LSTM\n",
        "feature_extractor = Model(inputs=lstm_model.input, outputs=lstm_model.get_layer(index=0).output)\n",
        "X_train_lstm_features = feature_extractor.predict(X_train)\n",
        "X_test_lstm_features = feature_extractor.predict(X_test)\n",
        "\n",
        "# Flatten features for XGBoost\n",
        "X_train_hybrid = X_train_lstm_features.reshape(X_train_lstm_features.shape[0], -1)\n",
        "X_test_hybrid = X_test_lstm_features.reshape(X_test_lstm_features.shape[0], -1)\n",
        "\n",
        "# Train XGBoost on LSTM features\n",
        "xgb_hybrid = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=7,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_hybrid.fit(X_train_hybrid, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_hybrid = xgb_hybrid.predict(X_test_hybrid)\n",
        "mae_hybrid = mean_absolute_error(y_test, y_pred_hybrid)\n",
        "print(f\"Hybrid Model MAE: {mae_hybrid:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💡 **Performance Comparison:**\n",
            "📌 XGBoost-Only MAE: 0.0407\n",
            "📌 LSTM-Only MAE: 0.0882\n",
            "📌 Hybrid (LSTM + XGBoost) MAE: 0.0407\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# ✅ 6. Compare Performance\n",
        "# ==============================\n",
        "print(\"\\n💡 **Performance Comparison:**\")\n",
        "print(f\"📌 XGBoost-Only MAE: {mae_xgb:.4f}\")\n",
        "print(f\"📌 LSTM-Only MAE: {mae_lstm:.4f}\")\n",
        "print(f\"📌 Hybrid (LSTM + XGBoost) MAE: {mae_hybrid:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
